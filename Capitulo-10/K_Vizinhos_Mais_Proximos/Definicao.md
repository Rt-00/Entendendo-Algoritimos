# üìñ K-Nearest Neighbors (K-NN) - K-Visinhos Mais Pr√≥ximos

## ‚úÖ Defini√ß√£o

O **K-Nearest Neighbors (K-NN)** √© um algoritmo de classifica√ß√£o e regress√£o baseado na similaridade entre os dados. Ao receber uma nova amostra, o K-NN procura os **K pontos de dados mais pr√≥ximos** (vizinhos) no conjunto de treinamento e, com base nesses, **prediz a classe** ou valor da nova amostra.

---

## ‚úÖ Quando usar?

* Quando √© necess√°rio fazer classifica√ß√µes ou previs√µes **n√£o-param√©tricas**, ou seja, sem assumir uma distribui√ß√£o pr√©via.
* Ideal para problemas onde a **proximidade ou similaridade** entre inst√¢ncias √© relevante.
* Funciona bem para bases de dados **pequenas ou moderadas**.

---

## ‚úÖ Como funciona?

1. **Escolher o valor de K**: o n√∫mero de vizinhos a serem considerados.
2. **Calcular a dist√¢ncia** entre a amostra de teste e todas as amostras do conjunto de treinamento.
   Dist√¢ncias comuns:

   * Euclidiana
   * Manhattan
   * Minkowski
3. **Selecionar os K vizinhos** com menor dist√¢ncia.
4. **Classifica√ß√£o**: a classe mais frequente entre os vizinhos.
5. **Regress√£o**: m√©dia ou pondera√ß√£o dos valores dos vizinhos.

---

## ‚úÖ Estruturas fundamentais

* **Dist√¢ncia**: Fun√ß√£o que calcula a diferen√ßa ou semelhan√ßa.
* **Lista de vizinhos**: estrutura para armazenar os K vizinhos mais pr√≥ximos.

---

## ‚úÖ Complexidade

| Aspecto       | Complexidade                                                  |
| ------------- | ------------------------------------------------------------- |
| Tempo (busca) | O(n \* d), onde n = n√∫mero de pontos, d = n√∫mero de dimens√µes |
| Espa√ßo        | O(n \* d)                                                     |

---

## ‚úÖ Vantagens

* ‚úîÔ∏è Simples de implementar.
* ‚úîÔ∏è N√£o precisa de treinamento expl√≠cito.
* ‚úîÔ∏è Funciona bem para classes bem separadas.

---

## ‚ùå Desvantagens

* ‚ùå Custo computacional alto com grandes volumes de dados.
* ‚ùå Sens√≠vel √† escolha de K.
* ‚ùå Pode ser afetado por dados irrelevantes ou ru√≠do.

---

## ‚úÖ Aplica√ß√µes pr√°ticas

* Sistemas de recomenda√ß√£o.
* Diagn√≥sticos m√©dicos.
* Reconhecimento de padr√µes.
* Detec√ß√£o de fraudes.

---

## ‚úÖ Escolha do valor de K

* **K pequeno** ‚Üí mais sens√≠vel ao ru√≠do.
* **K grande** ‚Üí mais robusto, mas pode incluir classes irrelevantes.

---

## ‚úÖ Dicas

* Normalizar os dados antes de usar K-NN.
* Testar diferentes m√©tricas de dist√¢ncia.
* Usar t√©cnicas de otimiza√ß√£o como KD-Tree para reduzir complexidade.